#!/usr/bin/env python
import os
import numpy as np
import json
from datetime import datetime
from tqdm import tqdm
import sys
import logging
from multiprocessing import Pool, cpu_count  # Add this import
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tabulate import tabulate
from colorama import Fore, Style, init
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix


init(autoreset=True)

# Bu·ªôc s·ª≠ d·ª•ng CPU
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# Th√™m c√°c bi·∫øn m√¥i tr∆∞·ªùng tr∆∞·ªõc khi t·∫°o m√¥ h√¨nh
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # T·∫Øt c·∫£nh b√°o t·ªëi ∆∞u h√≥a oneDNN

# Ki·ªÉm tra v√† c·∫•u h√¨nh GPU/CPU
def setup_gpu():
    """Check and configure GPU if available using TensorFlow"""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"\n{Fore.GREEN}ƒê√£ t√¨m th·∫•y GPU:{Style.RESET_ALL}")
            for gpu in gpus:
                print(f"{Fore.CYAN}- {gpu.name}")
            return True
        except RuntimeError as e:
            print(f"\n{Fore.YELLOW}L·ªói khi c·∫•u h√¨nh GPU: {e}{Style.RESET_ALL}")
    print(f"\n{Fore.YELLOW}Kh√¥ng t√¨m th·∫•y GPU, s·ª≠ d·ª•ng CPU...{Style.RESET_ALL}")
    return False

# ƒê∆∞·ªùng d·∫´n ƒë·∫ßu v√†o v√† ƒë·∫ßu ra
DATA_PATH = 'Data'  # Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
MODEL_PATH = 'Models'  # Th∆∞ m·ª•c ch·ª©a model ƒë√£ train
LOG_PATH = 'Logs'  # Th∆∞ m·ª•c ch·ª©a logs

def setup_logging():
    """Simplified logging setup"""
    os.makedirs(LOG_PATH, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(LOG_PATH, f'training_{timestamp}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logging.info("=== TRAINING STARTED ===")
    logging.info(f"Log file: {log_file}")

def load_action_mapping():
    """Load mapping t·ª´ file json"""
    mapping_file = os.path.join('Logs', 'action_mapping.json')  # Changed from 'Models' to 'Logs'
    try:
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
            print(f"\n{Fore.GREEN}ƒê√£ t·∫£i mapping:{Style.RESET_ALL}")
            print(f"{Fore.CYAN}- Ng√†y t·∫°o: {mapping_data['created_date']}")
            print(f"{Fore.CYAN}- T·ªïng s·ªë h√†nh ƒë·ªông: {mapping_data['total_actions']}")
            return mapping_data['actions']
    except Exception as e:
        print(f"{Fore.RED}L·ªói khi t·∫£i mapping: {str(e)}{Style.RESET_ALL}")
        return None

def save_action_mapping(selected_actions, log_path='Logs'):
    """L∆∞u mapping c·ªßa c√°c h√†nh ƒë·ªông ƒë√£ ch·ªçn"""
    os.makedirs(log_path, exist_ok=True)
    mapping_file = os.path.join(log_path, 'action_mapping.json')
    
    mapping = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'created_date': datetime.now().strftime('%Y-%m-%d'),  # Th√™m tr∆∞·ªùng n√†y
        'total_actions': len(selected_actions),
        'actions': {
            action: {
                'processed_name': convert_to_ascii(action),
                'original_name': action
            }
            for action in selected_actions
        }
    }
    
    with open(mapping_file, 'w', encoding='utf-8') as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)

def get_actions_from_directory():
    """L·∫•y danh s√°ch h√†nh ƒë·ªông v·ªõi x·ª≠ l√Ω l·ªói t·ªët h∆°n"""
    # Ki·ªÉm tra v√† t·∫°o th∆∞ m·ª•c Data n·∫øu ch∆∞a t·ªìn t·∫°i
    if not os.path.exists(DATA_PATH):
        try:
            os.makedirs(DATA_PATH)
            print(f"{Fore.YELLOW}ƒê√£ t·∫°o th∆∞ m·ª•c Data{Style.RESET_ALL}")
        except Exception as e:
            print(f"{Fore.RED}Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c Data: {str(e)}{Style.RESET_ALL}")
            return None

    # Th·ª≠ t·∫£i mapping tr∆∞·ªõc
    try:
        mapping_file = os.path.join('Logs', 'action_mapping.json')
        if os.path.exists(mapping_file):
            with open(mapping_file, 'r', encoding='utf-8') as f:
                mapping_data = json.load(f)
                if 'actions' in mapping_data:
                    actions = sorted([info['processed_name'] 
                                   for info in mapping_data['actions'].values()])
                    print(f"\n{Fore.GREEN}ƒê√£ t·∫£i {len(actions)} h√†nh ƒë·ªông t·ª´ mapping{Style.RESET_ALL}")
                    return np.array(actions)
    except Exception as e:
        print(f"{Fore.YELLOW}L·ªói khi ƒë·ªçc mapping: {str(e)}{Style.RESET_ALL}")

    # Qu√©t th∆∞ m·ª•c Data n·∫øu kh√¥ng c√≥ mapping
    try:
        actions = []
        for item in os.listdir(DATA_PATH):
            if os.path.isdir(os.path.join(DATA_PATH, item)):
                actions.append(item)
        
        if not actions:
            print(f"{Fore.YELLOW}Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c con trong Data/{Style.RESET_ALL}")
            return None
            
        print(f"\n{Fore.GREEN}ƒê√£ t√¨m th·∫•y {len(actions)} h√†nh ƒë·ªông t·ª´ th∆∞ m·ª•c:{Style.RESET_ALL}")
        for idx, action in enumerate(sorted(actions), 1):
            print(f"{Fore.CYAN}{idx}. {action}")
            
        return np.array(sorted(actions))
        
    except Exception as e:
        print(f"{Fore.RED}L·ªói khi qu√©t th∆∞ m·ª•c Data: {str(e)}{Style.RESET_ALL}")
        return None

actions = get_actions_from_directory()
if actions is None:
    print("Kh√¥ng th·ªÉ t·∫£i danh s√°ch nh√£n. Ch∆∞∆°ng tr√¨nh s·∫Ω k·∫øt th√∫c.")
    sys.exit(1)

no_sequences = 60
sequence_length = 60

def build_model():
    """X√¢y d·ª±ng m√¥ h√¨nh LSTM Keras"""
    # T·∫°o l·ªõp ƒë·∫ßu v√†o r√µ r√†ng
    inputs = tf.keras.Input(shape=(60, 126))
    
    # Kh·ªëi LSTM th·ª© nh·∫•t
    x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.3))(inputs)
    x = BatchNormalization()(x)
    
    # Kh·ªëi LSTM th·ª© hai
    x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.3))(x)
    x = BatchNormalization()(x)
    
    # Kh·ªëi LSTM th·ª© ba
    x = Bidirectional(LSTM(256, dropout=0.3))(x)
    x = BatchNormalization()(x)
    
    # C√°c l·ªõp Dense
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = BatchNormalization()(x)
    
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = BatchNormalization()(x)
    
    # L·ªõp ƒë·∫ßu ra
    outputs = Dense(len(actions), activation='softmax')(x)
    
    # T·∫°o m√¥ h√¨nh
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    
    # Bi√™n d·ªãch m√¥ h√¨nh
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def load_latest_checkpoint():
    """Load latest Keras checkpoint with full training state"""
    checkpoint_path = os.path.join('Models', 'checkpoints', 'checkpoint_model.keras')
    metadata_path = os.path.join('Models', 'checkpoints', 'training_metadata.json')
    
    if os.path.exists(checkpoint_path) and os.path.exists(metadata_path):
        try:
            # Load Keras model
            keras_model = tf.keras.models.load_model(checkpoint_path)
            
            # Load metadata
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            
            # Convert Keras model to PyTorch format
            model = build_model()
            # ... (weights transfer logic)
            
            training_state = {
                'epoch': metadata['epoch'],
                'best_accuracy': metadata['best_accuracy'],
                'history': metadata.get('history', None),
                'prev_val_loss': metadata.get('prev_val_loss', float('inf')),
                'table_data': metadata.get('table_data', [])
            }
            
            print(f"\nƒê√£ t√¨m th·∫•y checkpoint:")
            print(f"- Epoch: {training_state['epoch'] + 1}")
            print(f"- ƒê·ªô ch√≠nh x√°c t·ªët nh·∫•t: {training_state['best_accuracy']:.4%}")
            
            return model, training_state
            
        except Exception as e:
            print(f"L·ªói khi t·∫£i checkpoint: {str(e)}")
            return None, None
    return None, None

def save_checkpoint(model, epoch, best_accuracy, history, table_data, is_best=False):
    """Save Keras checkpoint"""
    os.makedirs(os.path.join(MODEL_PATH, 'checkpoints'), exist_ok=True)
    
    # Save model
    checkpoint_path = os.path.join(MODEL_PATH, 'checkpoints', f'checkpoint_model_{epoch:03d}.keras')
    model.save(checkpoint_path)
    
    # Save metadata
    metadata = {
        'epoch': epoch,
        'best_accuracy': float(best_accuracy),
        'history': history,
        'table_data': table_data
    }
    
    metadata_path = os.path.join(MODEL_PATH, 'checkpoints', 'training_metadata.json')
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f)
        
    if is_best:
        best_path = os.path.join(MODEL_PATH, 'best_model.keras')
        model.save(best_path)

def load_sequence_chunk(args):
    """Load a chunk of sequences in parallel v·ªõi ki·ªÉm tra chi ti·∫øt"""
    action, sequence_range = args
    sequences = []
    labels = []
    
    try:
        # Ki·ªÉm tra th∆∞ m·ª•c action c√≥ t·ªìn t·∫°i kh√¥ng
        action_path = os.path.join(DATA_PATH, action)
        if not os.path.exists(action_path):
            logging.error(f"Th∆∞ m·ª•c h√†nh ƒë·ªông kh√¥ng t·ªìn t·∫°i: {action_path}")
            return sequences, labels

        # In th√¥ng tin debug
        logging.info(f"\nƒêang x·ª≠ l√Ω h√†nh ƒë·ªông: {action}")
        logging.info(f"ƒê∆∞·ªùng d·∫´n: {action_path}")
        logging.info(f"D·∫£i sequence c·∫ßn x·ª≠ l√Ω: {list(sequence_range)}")
        
        # Li·ªát k√™ t·∫•t c·∫£ sequence c√≥ s·∫µn
        available_sequences = [s for s in os.listdir(action_path) 
                             if os.path.isdir(os.path.join(action_path, s))]
        logging.info(f"C√°c sequence c√≥ s·∫µn: {available_sequences}")

        for sequence_idx in sequence_range:
            sequence_path = os.path.join(action_path, str(sequence_idx))
            
            # Ki·ªÉm tra chi ti·∫øt sequence
            if not os.path.exists(sequence_path):
                logging.warning(f"B·ªè qua sequence {sequence_idx} - Kh√¥ng t·ªìn t·∫°i ƒë∆∞·ªùng d·∫´n: {sequence_path}")
                continue

            # Ki·ªÉm tra v√† load frames
            frame_files = sorted([f for f in os.listdir(sequence_path) 
                                if f.endswith('.npy')],
                               key=lambda x: int(x.split('.')[0]))
            
            logging.info(f"Sequence {sequence_idx}: T√¨m th·∫•y {len(frame_files)} frames")
            
            if len(frame_files) != sequence_length:
                logging.error(f"B·ªè qua sequence {sequence_idx}: Kh√¥ng ƒë·ªß frames ({len(frame_files)}/{sequence_length})")
                continue

            # Load v√† ki·ªÉm tra t·ª´ng frame
            window = []
            valid_sequence = True
            
            for frame_file in frame_files:
                file_path = os.path.join(sequence_path, frame_file)
                try:
                    frame_data = np.load(file_path, allow_pickle=True)
                    if frame_data.shape != (126,):
                        logging.error(f"Frame kh√¥ng ƒë√∫ng k√≠ch th∆∞·ªõc: {file_path} ({frame_data.shape})")
                        valid_sequence = False
                        break
                    window.append(frame_data)
                except Exception as e:
                    logging.error(f"L·ªói load frame {file_path}: {str(e)}")
                    valid_sequence = False
                    break

            if valid_sequence and len(window) == sequence_length:
                sequences.append(window)
                labels.append(actions.tolist().index(action))
                logging.info(f"ƒê√£ load th√†nh c√¥ng sequence {sequence_idx}")
            else:
                logging.warning(f"B·ªè qua sequence {sequence_idx} do kh√¥ng h·ª£p l·ªá")

    except Exception as e:
        logging.error(f"L·ªói x·ª≠ l√Ω h√†nh ƒë·ªông {action}: {str(e)}", exc_info=True)
    
    logging.info(f"K·∫øt th√∫c x·ª≠ l√Ω {action}: {len(sequences)} sequences h·ª£p l·ªá")
    return sequences, labels

def prepare_training_data():
    """Optimized data preparation using multiprocessing"""
    logging.info("B·∫Øt ƒë·∫ßu chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán")
    
    try:
        # T·∫°o header cho b·∫£ng
        headers = ['STT', 'H√†nh ƒë·ªông', 'Ti·∫øn ƒë·ªô', 'S·ªë m·∫´u', 'Tr·∫°ng th√°i']
        table_data = []
        all_sequences = []
        all_labels = []
        
        print("ƒêang qu√©t d·ªØ li·ªáu training...")
        total_sequences = 0
        total_frames = 0
        
        # Ki·ªÉm tra s·ªë l∆∞·ª£ng d·ªØ li·ªáu
        for action in actions:
            action_path = os.path.join(DATA_PATH, action)
            if not os.path.exists(action_path):
                logging.error(f"Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {action_path}")
                continue
                
            sequences = [s for s in os.listdir(action_path) 
                        if os.path.isdir(os.path.join(action_path, s))]
            total_sequences += len(sequences)
            
            for seq in sequences:
                seq_path = os.path.join(action_path, seq)
                frames = [f for f in os.listdir(seq_path) if f.endswith('.npy')]
                total_frames += len(frames)
        
        print(f"T·ªïng s·ªë sequences: {total_sequences}")
        print(f"T·ªïng s·ªë frames: {total_frames}")
        
        if total_sequences == 0:
            logging.error("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu training!")
            return None, None, None, None  # Tr·∫£ v·ªÅ 4 gi√° tr·ªã None thay v√¨ 2
        
        # Chia nh·ªè c√¥ng vi·ªác cho multiprocessing
        chunk_size = 10  # S·ªë sequence x·ª≠ l√Ω m·ªói l·∫ßn
        num_processes = max(1, cpu_count() - 1)  # Gi·ªØ l·∫°i 1 core cho h·ªá th·ªëng
        
        print(f"S·ª≠ d·ª•ng {num_processes} processes ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu")
        
        with Pool(processes=num_processes) as pool:
            for action_idx, action in enumerate(actions):
                # Chia sequences th√†nh c√°c chunks
                sequence_chunks = [
                    (action, range(i, min(i + chunk_size, no_sequences)))
                    for i in range(0, no_sequences, chunk_size)
                ]
                
                # X·ª≠ l√Ω song song c√°c chunks
                results = list(tqdm(
                    pool.imap(load_sequence_chunk, sequence_chunks),
                    total=len(sequence_chunks),
                    desc=f"X·ª≠ l√Ω '{action}'",
                    leave=False
                ))
                
                # Gom k·∫øt qu·∫£
                action_sequences = []
                action_labels = []
                for seq, lab in results:
                    action_sequences.extend(seq)
                    action_labels.extend(lab)
                
                # C·∫≠p nh·∫≠t b·∫£ng tr·∫°ng th√°i
                progress = f"{len(action_sequences)}/{no_sequences}"
                status = "‚úì" if len(action_sequences) == no_sequences else "‚ö†"
                table_data.append([
                    f"{action_idx + 1}/{len(actions)}",
                    action,
                    progress,
                    len(action_sequences),
                    status
                ])
                
                # Th√™m v√†o t·ªïng h·ª£p
                all_sequences.extend(action_sequences)
                all_labels.extend(action_labels)
                
                # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô
                os.system('cls' if os.name == 'nt' else 'clear')
                print("\nTi·∫øn ƒë·ªô chu·∫©n b·ªã d·ªØ li·ªáu:")
                print(tabulate(table_data, headers=headers, tablefmt='grid'))
                print(f"\nT·ªïng s·ªë m·∫´u hi·ªán t·∫°i: {len(all_sequences)}")
    
        # Ki·ªÉm tra d·ªØ li·ªáu tr∆∞·ªõc khi tr·∫£ v·ªÅ
        if len(all_sequences) == 0:
            logging.error("Kh√¥ng c√≥ d·ªØ li·ªáu sau khi x·ª≠ l√Ω!")
            return None, None, None, None
            
        # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu
        X = np.array(all_sequences)
        y = to_categorical(all_labels).astype(int)
        
        return train_test_split(X, y, test_size=0.2, random_state=42)
        
    except Exception as e:
        logging.error(f"L·ªói khi chu·∫©n b·ªã d·ªØ li·ªáu: {str(e)}")
        return None, None, None, None  # Tr·∫£ v·ªÅ 4 gi√° tr·ªã None thay v√¨ 2

def train_model(epochs=50, batch_size=32, initial_epoch=0):
    """Train model using Keras with better monitoring"""
    # Setup model
    model = build_model()
    model.best_accuracy = 0
    
    # Setup callbacks
    table_data = []
    headers = ['Epoch', 'Loss', 'Accuracy', 'Val Loss', 'Val Acc', 'LR', 'Status']
    
    callbacks = [
        ModelCheckpoint(
            os.path.join('Models', 'checkpoints', 'model_{epoch:02d}.keras'),
            save_best_only=True,
            monitor='val_accuracy'
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=5,
            min_lr=0.00001
        ),
        TrainingCallback(table_data, headers)
    ]
    
    # Prepare data v·ªõi ki·ªÉm tra l·ªói
    try:
        X_train, X_test, y_train, y_test = prepare_training_data()
        if X_train is None or X_test is None or y_train is None or y_test is None:
            logging.error("Kh√¥ng th·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu training")
            return None, None
        
        # Train model
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=epochs,
            batch_size=batch_size,
            initial_epoch=initial_epoch,
            callbacks=callbacks,
            verbose=0  # T·∫Øt output m·∫∑c ƒë·ªãnh
        )
        
        # Save final model
        save_model(model)
        
        return history.history, model
        
    except KeyboardInterrupt:
        print("\nHu·∫•n luy·ªán b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        return history.history, model
    except Exception as e:
        logging.error(f"L·ªói trong qu√° tr√¨nh hu·∫•n luy·ªán: {str(e)}")
        return None, None

# Th√™m class TensorBoard callback
class TrainingCallback(tf.keras.callbacks.Callback):
    def __init__(self, table_data, headers):
        super().__init__()
        self.table_data = table_data
        self.headers = headers
        
    def on_epoch_end(self, epoch, logs=None):
        # C·∫≠p nh·∫≠t b·∫£ng tr·∫°ng th√°i
        status = ''
        if logs.get('val_accuracy', 0) > self.model.best_accuracy:
            status += f"{Fore.RED}üî•{Style.RESET_ALL}"
            self.model.best_accuracy = logs['val_accuracy']
        elif logs.get('val_accuracy', 0) >= self.model.best_accuracy * 0.95:
            status += f"{Fore.GREEN}‚úÖ{Style.RESET_ALL}"
        else:
            status += f"{Fore.YELLOW}‚ö†Ô∏è{Style.RESET_ALL}"
        
        # L·∫•y learning rate hi·ªán t·∫°i
        current_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))
            
        self.table_data.append([
            f'{epoch+1}/{self.params["epochs"]}',
            f'{logs["loss"]:.4f}',
            f'{logs["accuracy"]:.4%}',
            f'{logs["val_loss"]:.4f}',
            f'{logs["val_accuracy"]:.4%}',
            f'{current_lr:.6f}',  # S·ª≠ d·ª•ng learning rate ƒë√£ l·∫•y
            status
        ])
        
        # Hi·ªÉn th·ªã b·∫£ng
        os.system('cls' if os.name == 'nt' else 'clear')
        print(f"\n{Fore.GREEN}K·∫øt qu·∫£ hu·∫•n luy·ªán:{Style.RESET_ALL}")
        print(tabulate(self.table_data, self.headers, tablefmt='grid'))

def save_model(model, path='Models/final_model.keras'):
    """L∆∞u m√¥ h√¨nh Keras"""
    model.save(path)
    print(f"ƒê√£ l∆∞u m√¥ h√¨nh v√†o {path}")

def load_model(path='Models/best_model.keras'):
    """T·∫£i m√¥ h√¨nh Keras"""
    return tf.keras.models.load_model(path)

def plot_training_history(history):
    """Plot training metrics from dictionary history"""
    try:
        # Ki·ªÉm tra d·ªØ li·ªáu tr∆∞·ªõc khi v·∫Ω
        if not all(key in history for key in ['loss', 'accuracy', 'val_loss', 'val_accuracy']):
            print("Thi·∫øu d·ªØ li·ªáu history!")
            return
            
        if len(history['loss']) == 0:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu training ƒë·ªÉ v·∫Ω!")
            return

        plt.figure(figsize=(12, 4))
        
        # Plot v·ªõi nhi·ªÅu th√¥ng tin h∆°n
        epochs = range(1, len(history['loss']) + 1)
        
        # Plot accuracy
        plt.subplot(1, 2, 1)
        train_acc = plt.plot(epochs, history['accuracy'], 'b-', label='ƒê·ªô ch√≠nh x√°c hu·∫•n luy·ªán')
        val_acc = plt.plot(epochs, history['val_accuracy'], 'r-', label='ƒê·ªô ch√≠nh x√°c ki·ªÉm th·ª≠')
        plt.title('ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh')
        plt.xlabel('Epoch')
        plt.ylabel('ƒê·ªô ch√≠nh x√°c')
        plt.grid(True)
        plt.legend()
        
        # Plot loss
        plt.subplot(1, 2, 2)
        train_loss = plt.plot(epochs, history['loss'], 'b-', label='M·∫•t m√°t hu·∫•n luy·ªán')
        val_loss = plt.plot(epochs, history['val_loss'], 'r-', label='M·∫•t m√°t ki·ªÉm th·ª≠')
        plt.title('ƒê·ªô m·∫•t m√°t c·ªßa m√¥ h√¨nh')
        plt.xlabel('Epoch')
        plt.ylabel('M·∫•t m√°t')
        plt.grid(True)
        plt.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join('Logs', 'training_history.png'))
        plt.close()
        
        # In th√¥ng tin ƒë·ªÉ debug
        print("\nTh·ªëng k√™ training:")
        print(f"Epochs ƒë√£ train: {len(epochs)}")
        print(f"ƒê·ªô ch√≠nh x√°c cu·ªëi: {history['accuracy'][-1]:.4f}")
        print(f"Loss cu·ªëi: {history['loss'][-1]:.4f}")
        print("ƒê√£ l∆∞u bi·ªÉu ƒë·ªì hu·∫•n luy·ªán v√†o th∆∞ m·ª•c Logs")
        
    except Exception as e:
        print(f"L·ªói khi v·∫Ω bi·ªÉu ƒë·ªì: {str(e)}")
        # Print th√™m th√¥ng tin debug
        print("History dict:", history.keys())
        print("Lengths:", {k: len(v) for k, v in history.items()})

def evaluate_model(model, X_test, y_test):
    """Evaluate model v·ªõi t√™n h√†nh ƒë·ªông g·ªëc"""
    # Get predictions
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_test_classes = np.argmax(y_test, axis=1)
    
    # Calculate metrics
    acc = accuracy_score(y_test_classes, y_pred_classes)
    conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)
    
    # Print results
    print("\nK·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh:")
    print(f"ƒê·ªô ch√≠nh x√°c: {acc:.4f}")
    print("\nMa tr·∫≠n nh·∫ßm l·∫´n:")
    print(conf_matrix)
    
    # Th√™m t√™n g·ªëc v√†o k·∫øt qu·∫£ n·∫øu c√≥ mapping
    mapping = load_action_mapping()
    if mapping:
        # T·∫°o mapping ng∆∞·ª£c t·ª´ t√™n ƒë√£ x·ª≠ l√Ω v·ªÅ t√™n g·ªëc
        reverse_mapping = {
            v['processed_name']: k 
            for k, v in mapping.items()
        }
        
        # In k·∫øt qu·∫£ v·ªõi c·∫£ t√™n g·ªëc
        print("\nK·∫øt qu·∫£ theo t√™n g·ªëc:")
        for i, action in enumerate(actions):
            original = reverse_mapping.get(action, action)
            print(f"{i+1}. {action} ({original})")
            # In c√°c metric kh√°c...
    
    # Save model metrics
    with open(os.path.join('Logs', 'model_evaluation.txt'), 'w') as f:
        f.write(f"K·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh:\n")
        f.write(f"ƒê·ªô ch√≠nh x√°c: {acc:.4f}\n")
        f.write(f"\nMa tr·∫≠n nh·∫ßm l·∫´n:\n")
        f.write(str(conf_matrix))

if __name__ == "__main__":
    # Kh·ªüi t·∫°o logging
    setup_logging()
    
    try:
        logging.info("=== KH·ªûI T·∫†O HU·∫§N LUY·ªÜN ===")
        print(f"{Fore.GREEN}=== H·ªÜ TH·ªêNG NH·∫¨N D·∫†NG NG√îN NG·ªÆ K√ù HI·ªÜU ==={Style.RESET_ALL}")
        
        # Ki·ªÉm tra GPU
        has_gpu = setup_gpu()
        
        # Load actions v·ªõi mapping
        actions = get_actions_from_directory()
        if actions is None:
            print("Kh√¥ng th·ªÉ t·∫£i danh s√°ch h√†nh ƒë·ªông. Ch∆∞∆°ng tr√¨nh s·∫Ω k·∫øt th√∫c.")
            sys.exit(1)
        
        # Th·ª≠ t·∫£i checkpoint v·ªõi gi√° tr·ªã tr·∫£ v·ªÅ ƒë√£ s·ª≠a
        loaded_model, loaded_state = load_latest_checkpoint()
        
        if loaded_model is not None and loaded_state is not None:
            print(f"Ti·∫øp t·ª•c hu·∫•n luy·ªán t·ª´ epoch {loaded_state['epoch'] + 1}")
            model = loaded_model
            initial_epoch = loaded_state['epoch'] + 1
        else:
            print("\nKh·ªüi t·∫°o m√¥ h√¨nh m·ªõi...")
            model = build_model()
            initial_epoch = 0
        
        epochs = 50
        batch_size = 8 if has_gpu else 32
        
        train_history, trained_model = train_model(epochs=epochs, batch_size=batch_size, initial_epoch=initial_epoch)
        
        if trained_model is not None and train_history is not None:
            # Plot training history
            plot_training_history(train_history)
            print(f"{Fore.GREEN}ƒê√£ l∆∞u m√¥ h√¨nh v√† bi·ªÉu ƒë·ªì hu·∫•n luy·ªán th√†nh c√¥ng!{Style.RESET_ALL}")
            
    except Exception as e:
        logging.error(f"L·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c: {str(e)}", exc_info=True)
        print(f"{Fore.RED}L·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c: {str(e)}{Style.RESET_ALL}")
        sys.exit(1)

